# -*- coding: utf-8 -*-
"""Bản sao của flowers_recognition_inceptionv3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/189CFtwO4o85mFRBjwAytxbSr632Z4cWD

## Setup
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow.keras import layers, Sequential
from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.preprocessing import LabelEncoder

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from matplotlib import style
import seaborn as sns
# %matplotlib inline
style.use('fivethirtyeight')
sns.set(style='whitegrid',color_codes=True)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

import os
import random
import random as rn
import warnings
warnings.filterwarnings('ignore')

# Set image_size and batch_size
IMAGE_SIZE = (224, 224)
BATCH_SIZE = 32

# Target Directory
directory = "/content/drive/MyDrive/Dataset/"

# Train Data
train_data = tf.keras.preprocessing.image_dataset_from_directory(
             directory,
             subset='training',
             validation_split=0.2,
             image_size=IMAGE_SIZE,
             batch_size=BATCH_SIZE,
             seed=42)

# Valid data
valid_data = tf.keras.preprocessing.image_dataset_from_directory(
            directory,
            subset='validation',
            validation_split=0.2,
            image_size=IMAGE_SIZE,
            batch_size=BATCH_SIZE,
             seed=42)

#Dua du lieu ve dang bang
def count_exp(path, set_):
    dict_ = {}
    for expression in os.listdir(path):
        dir_ = path + expression
        dict_[expression] = len(os.listdir(dir_))-1
    df = pd.DataFrame(dict_, index=[set_])
    return df 
data_count = count_exp(directory, 'data')
#Bang
data_count.transpose().plot(kind='bar', figsize=(20,10))

class_names = train_data.class_names
class_names

label_encode = LabelEncoder()
class_names_label_encode = label_encode.fit_transform(class_names)
class_names_label_encode

"""## Hình dung hình ảnh"""

img = plt.imread("/content/drive/MyDrive/Dataset/HoaCamChuong/HoaCamChuong10.jpg")
plt.imshow(img)
plt.title("HoaCamChuong")
plt.axis("off")
plt.show();

img = plt.imread("/content/drive/MyDrive/Dataset/HoaDongTien/HoaDongTien (11).jpg")
plt.imshow(img)
plt.title("HoaDongTien")
plt.axis("off")
plt.show();

"""## Tạo chức năng Preprocess_image

Hàm trở về kiểu `float32` và chia tỷ lệ giữa 0 & 1 với image_shape = 224

muốn biết Image data preprocessing là cái gì thì [bấm dô đây](https://keras.io/api/preprocessing/image/)
"""

def preprocess_image(image, label, image_shape=224):
    
    img = tf.image.resize(image, [image_shape, image_shape])
    img = img/225.
    
    return tf.cast(img, tf.float32), label

preprocess_image(image=img, label='HoaDongTien')

"""## Batch & Prefetch"""

# prerocess_image thành train_data
train_data = train_data.map(map_func=preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
# ran the data
train_data = train_data.shuffle(buffer_size=1000).prefetch(buffer_size=tf.data.AUTOTUNE)


# preprocess_image to valid_data
valid_data = valid_data.map(map_func=preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
# shuffle the data
valid_data = valid_data.shuffle(buffer_size=1000).prefetch(buffer_size=tf.data.AUTOTUNE)

train_data, valid_data

"""## Modelling

#### Simple Dense Model
"""

def plot_loss_curves(history):
    
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    accuracy = history.history['accuracy']
    val_accuracy = history.history['val_accuracy']

    epochs = range(len(history.history['loss']))

  # Plot loss
    plt.plot(epochs, loss, label='training_loss')
    plt.plot(epochs, val_loss, label='val_loss')
    plt.title('Loss')
    plt.xlabel('Epochs')
    plt.legend()

  # Plot accuracy
    plt.figure()
    plt.plot(epochs, accuracy, label='training_accuracy')
    plt.plot(epochs, val_accuracy, label='val_accuracy')
    plt.title('Accuracy')
    plt.xlabel('Epochs')
    plt.legend();

"""### Model_2"""

# Set random seed
tf.random.set_seed(42)

# model_2
model_2 = Sequential([
    Conv2D(filters=32, kernel_size=4, padding='same', activation='relu', input_shape=(224,224,3)),
    MaxPool2D(2,2),
    Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'),
    MaxPool2D(2,2),
    Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'),
    MaxPool2D(2,2),
    Conv2D(filters=128, kernel_size=5, padding='same', activation='relu'),
    MaxPool2D(2,2),
    Flatten(),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dense(len(class_names_label_encode), activation='softmax')
])

# Compile
model_2.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
               optimizer='adam',
               metrics=['accuracy'])

# Fit 
history_2 = model_2.fit(train_data,
                       epochs=10,
                       validation_data=valid_data)

#model summary
model_2.summary()

# plot 
plot_loss_curves(history_2)

"""## Model_3"""

# random seed
tf.random.set_seed(42)

# model_3
model_3 = Sequential([
    Conv2D(filters=32, kernel_size=4, padding='same', activation='relu', input_shape=(224,224,3)),
    MaxPool2D(2,2),
    Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'),
    MaxPool2D(2,2),
    Conv2D(filters=64, kernel_size=4, padding='same', activation='relu'),
    MaxPool2D(2,2),
    Conv2D(filters=128, kernel_size=5, padding='same', activation='relu'),
    MaxPool2D(3,3),
    Conv2D(filters=128, kernel_size=5, padding='same', activation='relu'),
    MaxPool2D(3,3),
    Conv2D(filters=128,kernel_size=5, padding='same', activation='relu'),
    MaxPool2D(3,3),
    Flatten(),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dense(len(class_names_label_encode), activation='softmax')
    
])

# compile
model_3.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
               optimizer=tf.keras.optimizers.Adam(),
               metrics=['accuracy'])

# fit 
history_3 = model_3.fit(train_data,
                       epochs=20,
                       validation_data=valid_data)

model_3.summary()

plot_loss_curves(history_3)

"""## Transfer Learning

### Inception V3
"""

# Download Inception V3 model
base_model_inception = tf.keras.applications.inception_v3.InceptionV3(include_top=False)

base_model_inception.trainable=False

# Inputs
inputs = tf.keras.layers.Input(shape=(224,224,3), name='input_layer')

# chia gia tri
x = tf.keras.layers.experimental.preprocessing.Rescaling(1/255.)(inputs)

# chuyen dau vao base_model
x = base_model_inception(inputs,training=False)

# GlobalAveragePooling2D
x = tf.keras.layers.GlobalAveragePooling2D()(x)

# outputs
outputs = tf.keras.layers.Dense(len(class_names_label_encode), activation='softmax')(x)

# Build model
model_4 = tf.keras.Model(inputs, outputs)

# Compile 
model_4.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
               optimizer='adam',
               metrics=['accuracy'])

model_4.summary()

history_4 = model_4.fit(train_data,
                       epochs=10,
                       validation_data=valid_data)

plot_loss_curves(history_4)

"""### Resnet50"""

baseline_model_resnet50 = tf.keras.applications.resnet50.ResNet50(include_top=False)


baseline_model_resnet50.trainable= False


inputs = tf.keras.layers.Input(shape=(224,224,3))


x = tf.keras.layers.experimental.preprocessing.Rescaling(1/255.)(inputs)


x = baseline_model_resnet50(inputs,training=False)


x = tf.keras.layers.GlobalAveragePooling2D()(x)


outputs = tf.keras.layers.Dense(len(class_names_label_encode),activation='softmax')(x)


model_5 = tf.keras.Model(inputs,outputs)

# Compile
model_5.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
               optimizer='adam',
               metrics=['accuracy'])

# Summary
model_5.summary()

# Fit the model
history_5 = model_5.fit(train_data,
                       epochs=20,
                       validation_data=valid_data)

# plotlossscurve
plot_loss_curves(history_5)

"""## EfficientnetB5"""

baseline_model_efficientnetb5 = tf.keras.applications.efficientnet.EfficientNetB5(include_top=False)


baseline_model_efficientnetb5.trainable = False


inputs = tf.keras.layers.Input(shape=(224,224,3))


x = baseline_model_efficientnetb5(inputs, training=False)


x = tf.keras.layers.GlobalAveragePooling2D()(x)


outputs = tf.keras.layers.Dense(len(class_names_label_encode), activation='softmax')(x)


model_6 = tf.keras.Model(inputs, outputs)


model_6.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
               optimizer='adam',
               metrics=['accuracy'])


model_6.summary()

# Fit the model
history_6 = model_6.fit(train_data,
                       epochs=20,
                       validation_data = valid_data)

# plotlossscurve
plot_loss_curves(history_6)

"""** Mô hình InceptionV3 hoạt động tốt khi so sánh với ResNet50 và EfficientNetB5 **

## Fine-tuning
"""

# Checking the layer of best forming model(inception)
for layer_number, layer in enumerate(base_model_inception.layers):
    print(layer_number, layer, layer.trainable)

base_model_inception.summary()

"""## kiến trúc inceptionV3 """

##  kiến ​​​​trúc inceptionv3
from tensorflow.keras.utils import plot_model
plot_model(base_model_inception)

base_model_inception.trainable = True

# đóng băng tất cả các lớp ngoại trừ 15 lớp cuối cùng
for layer in base_model_inception.layers[:-15]:
    layer.trainable = False

# Biên dịch lại mô hình
model_4.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),
               optimizer='adam',
               metrics=['accuracy'])

# Check the layers
for layer_number,layer in enumerate(base_model_inception.layers):
    print(layer_number, layer, layer.trainable)

# refit the model
history_model_4_fine_tune = model_4.fit(train_data,
                                       epochs=10,
                                       validation_data=valid_data,
                                       initial_epoch=history_4.epoch[-1])

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py

from helper_functions import compare_historys

compare_historys(history_4,
                history_model_4_fine_tune)

# save the best performing model
model_4.save("output/best_performing_model")
# model_4.save("output/mode.h5")

model_4.save("model/model.h5")

# load model
loaded_model = tf.keras.models.load_model("output/best_performing_model")
loaded_model
#flask api

# Evaluate
loaded_model.evaluate(valid_data)

# make prediction
pred_probs = loaded_model.predict(valid_data)
pred_probs[:10]

# pred classes
pred_class = pred_probs.argmax(axis=1)
pred_class[:10]

# Unbatch the images and labels
y_labels = []
for images, labels in valid_data.unbatch():
    y_labels.append(labels.numpy().argmax())
y_labels[:10]

!pip install tensorflowjs

!pip install ipython

!pip install PyInquirer

# !tensorflowjs
# đổi từ file h5 sang json
! tensorflowjs_converter --input_format=keras /content/model/model.h5 /content/model/tfjs_model

from google.colab import files
import pandas as pd
files.download('/content/model/tfjs_model')